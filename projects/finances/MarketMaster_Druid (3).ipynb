{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydruid.client import *\n",
    "from pydruid.utils.aggregators import *\n",
    "from pydruid.utils.postaggregator import *\n",
    "from pydruid.utils.filters import *\n",
    "from pydruid.utils.having import Having\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Groupby query getting # Records per Day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'version': 'v1', 'timestamp': '2019-08-25T00:00:00.000Z', 'event': {'num_rows': 7517}}, {'version': 'v1', 'timestamp': '2019-08-31T00:00:00.000Z', 'event': {'num_rows': 748}}, {'version': 'v1', 'timestamp': '2019-09-01T00:00:00.000Z', 'event': {'num_rows': 742}}, {'version': 'v1', 'timestamp': '2019-09-02T00:00:00.000Z', 'event': {'num_rows': 739}}, {'version': 'v1', 'timestamp': '2019-09-03T00:00:00.000Z', 'event': {'num_rows': 501}}, {'version': 'v1', 'timestamp': '2019-09-06T00:00:00.000Z', 'event': {'num_rows': 501}}, {'version': 'v1', 'timestamp': '2019-09-07T00:00:00.000Z', 'event': {'num_rows': 335}}, {'version': 'v1', 'timestamp': '2019-09-08T00:00:00.000Z', 'event': {'num_rows': 310}}, {'version': 'v1', 'timestamp': '2019-09-09T00:00:00.000Z', 'event': {'num_rows': 310}}, {'version': 'v1', 'timestamp': '2019-09-10T00:00:00.000Z', 'event': {'num_rows': 310}}, {'version': 'v1', 'timestamp': '2019-09-12T00:00:00.000Z', 'event': {'num_rows': 308}}]\n"
     ]
    }
   ],
   "source": [
    "query = PyDruid('http://bislave02.dev3.bloombergpolarlake.com:8082', 'druid/v2/')\n",
    " \n",
    " \n",
    "weekly_counts = query.groupby(\n",
    "    datasource = \"bpl_bdp_publication_dev3.bpl_market_master_druid\",\n",
    "    granularity = \"day\",\n",
    "    intervals = \"2018-05-01T00:00/2019-10-28T23:59\",\n",
    "    aggregations = {\"num_rows\": count(None)}\n",
    ")\n",
    " \n",
    "print (weekly_counts)  # Do this if you want to see the raw JSON"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export query result to Pandas DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    num_rows\n",
      "1       7517\n",
      "2        748\n",
      "3        742\n",
      "4        739\n",
      "5        501\n",
      "6        501\n",
      "7        335\n",
      "8        310\n",
      "9        310\n",
      "10       310\n",
      "11       308\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df = query.export_pandas() # Client will import Pandas, no need to do so separately.\n",
    " \n",
    "df = df.drop('timestamp', axis=1)  # Don't need the timestamp column here\n",
    " \n",
    "df.index = range(1, len(df)+1)  # Get a naturally numbered index\n",
    " \n",
    "print (df)\n",
    " \n",
    "#df.plot(x='num_rows', kind='bar')\n",
    " \n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Total number of records accross all time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'version': 'v1', 'timestamp': '2018-05-01T00:00:00.000Z', 'event': {'num_rows': 12321}}]\n"
     ]
    }
   ],
   "source": [
    " \n",
    "query = PyDruid('http://bislave02.dev3.bloombergpolarlake.com:8082', 'druid/v2/')\n",
    " \n",
    " \n",
    "weekly_counts = query.groupby(\n",
    "    datasource = \"bpl_bdp_publication_dev3.bpl_market_master_druid\",\n",
    "    granularity = \"all\",\n",
    "    intervals = \"2018-05-01T00:00/2020-08-28T23:59\",\n",
    "    aggregations = {\"num_rows\": count(None)}\n",
    ")\n",
    " \n",
    "print (weekly_counts)  # Do this if you want to see the raw JSON"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TopN Query to count the # records by bpl_classifier_i \n",
    "Note that TopN queries are an approximation only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'timestamp': '2019-08-25T00:00:00.000Z', 'result': [{'updates': 7053, 'bpl_classifier_i': 'E'}, {'updates': 3236, 'bpl_classifier_i': 'F'}, {'updates': 506, 'bpl_classifier_i': 'T'}, {'updates': 412, 'bpl_classifier_i': 'X'}, {'updates': 323, 'bpl_classifier_i': 'P'}, {'updates': 283, 'bpl_classifier_i': 'I'}, {'updates': 232, 'bpl_classifier_i': 'O'}, {'updates': 189, 'bpl_classifier_i': 'N'}, {'updates': 87, 'bpl_classifier_i': 'L'}]}]\n"
     ]
    }
   ],
   "source": [
    "\n",
    " \n",
    "query = PyDruid('http://bislave02.dev3.bloombergpolarlake.com:8082', 'druid/v2/')\n",
    " \n",
    " \n",
    "top_langs = query.topn(\n",
    "    datasource = \"bpl_bdp_publication_dev3.bpl_market_master_druid\",\n",
    "    granularity = \"all\",\n",
    "    intervals = \"2019-05-01T00:00/2019-10-28T23:59\",\n",
    "    dimension = \"bpl_classifier_i\",\n",
    "    aggregations = {\"updates\": count(\"*\")},\n",
    "    #filter=Dimension(\"id_bb_global\") == \"BBG00JPQ7J31\",\n",
    "    metric = \"updates\",\n",
    "    threshold = 10000\n",
    ")\n",
    " \n",
    "print (top_langs)  # Do this if you want to see the raw JSON"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Max ? broken down by bpl_classifier_i and bpl_classifier_ii by month\n",
    "## Nothing available to aggregate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "#query = PyDruid('http://bislave02.dev3.bloombergpolarlake.com:8082', 'druid/v2/')\n",
    " \n",
    " \n",
    "#weekly_counts = query.groupby(\n",
    "#    datasource = \"ssb_druid.bpl_pricing_druid_cube\",\n",
    "#    granularity = \"month\",\n",
    "#    intervals = \"2018-05-01T00:00/2018-08-28T23:59\",\n",
    "#    aggregations = {\"max_price\": doublemax(\"px_last\")},\n",
    "#    dimensions=[\"bpl_classifier_i\", \"bpl_classifier_ii\"]\n",
    "#)\n",
    " \n",
    "#print weekly_counts  # Do this if you want to see the raw JSON"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Post Aggregator Example \n",
    "Not a realistic example - just divides monthly count by 31"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'version': 'v1', 'timestamp': '2019-08-01T00:00:00.000Z', 'event': {'num_rows': 8265, 'average_daily': 266.61290322580646}}, {'version': 'v1', 'timestamp': '2019-09-01T00:00:00.000Z', 'event': {'num_rows': 4056, 'average_daily': 130.83870967741936}}]\n"
     ]
    }
   ],
   "source": [
    " \n",
    "query = PyDruid('http://bislave02.dev3.bloombergpolarlake.com:8082', 'druid/v2/')\n",
    " \n",
    " \n",
    "weekly_counts = query.groupby(\n",
    "    datasource = \"bpl_bdp_publication_dev3.bpl_market_master_druid\",\n",
    "    granularity = \"month\",\n",
    "    intervals = \"2018-05-01T00:00/2020-08-28T23:59\",\n",
    "    aggregations = {\"num_rows\": count(None)},\n",
    "    post_aggregations={\"average_daily\":(Field(\"num_rows\")/Const(31))}\n",
    ")\n",
    " \n",
    "print (weekly_counts)  # Do this if you want to see the raw JSON"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check are there any markets updated more than 4 times same week"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'version': 'v1', 'timestamp': '2019-09-02T00:00:00.000Z', 'event': {'id_bpl_market': '10000002', 'num_rows': 5}}, {'version': 'v1', 'timestamp': '2019-09-02T00:00:00.000Z', 'event': {'id_bpl_market': '10000036', 'num_rows': 5}}, {'version': 'v1', 'timestamp': '2019-09-02T00:00:00.000Z', 'event': {'id_bpl_market': '10000070', 'num_rows': 5}}, {'version': 'v1', 'timestamp': '2019-09-02T00:00:00.000Z', 'event': {'id_bpl_market': '10000344', 'num_rows': 5}}, {'version': 'v1', 'timestamp': '2019-09-02T00:00:00.000Z', 'event': {'id_bpl_market': '10000336', 'num_rows': 5}}, {'version': 'v1', 'timestamp': '2019-09-02T00:00:00.000Z', 'event': {'id_bpl_market': '10000328', 'num_rows': 5}}, {'version': 'v1', 'timestamp': '2019-09-02T00:00:00.000Z', 'event': {'id_bpl_market': '10000090', 'num_rows': 5}}, {'version': 'v1', 'timestamp': '2019-09-02T00:00:00.000Z', 'event': {'id_bpl_market': '10000084', 'num_rows': 5}}, {'version': 'v1', 'timestamp': '2019-09-02T00:00:00.000Z', 'event': {'id_bpl_market': '10000079', 'num_rows': 5}}, {'version': 'v1', 'timestamp': '2019-09-02T00:00:00.000Z', 'event': {'id_bpl_market': '10000076', 'num_rows': 5}}, {'version': 'v1', 'timestamp': '2019-09-02T00:00:00.000Z', 'event': {'id_bpl_market': '10000074', 'num_rows': 5}}, {'version': 'v1', 'timestamp': '2019-09-02T00:00:00.000Z', 'event': {'id_bpl_market': '10000072', 'num_rows': 5}}, {'version': 'v1', 'timestamp': '2019-09-02T00:00:00.000Z', 'event': {'id_bpl_market': '10000068', 'num_rows': 5}}, {'version': 'v1', 'timestamp': '2019-09-02T00:00:00.000Z', 'event': {'id_bpl_market': '10000066', 'num_rows': 5}}, {'version': 'v1', 'timestamp': '2019-09-02T00:00:00.000Z', 'event': {'id_bpl_market': '10000064', 'num_rows': 5}}, {'version': 'v1', 'timestamp': '2019-09-02T00:00:00.000Z', 'event': {'id_bpl_market': '10000062', 'num_rows': 5}}, {'version': 'v1', 'timestamp': '2019-09-02T00:00:00.000Z', 'event': {'id_bpl_market': '10000060', 'num_rows': 5}}, {'version': 'v1', 'timestamp': '2019-09-02T00:00:00.000Z', 'event': {'id_bpl_market': '10000056', 'num_rows': 5}}, {'version': 'v1', 'timestamp': '2019-09-02T00:00:00.000Z', 'event': {'id_bpl_market': '10000054', 'num_rows': 5}}, {'version': 'v1', 'timestamp': '2019-09-02T00:00:00.000Z', 'event': {'id_bpl_market': '10003088', 'num_rows': 5}}, {'version': 'v1', 'timestamp': '2019-09-02T00:00:00.000Z', 'event': {'id_bpl_market': '10000046', 'num_rows': 5}}, {'version': 'v1', 'timestamp': '2019-09-02T00:00:00.000Z', 'event': {'id_bpl_market': '10000040', 'num_rows': 5}}, {'version': 'v1', 'timestamp': '2019-09-02T00:00:00.000Z', 'event': {'id_bpl_market': '10000038', 'num_rows': 5}}, {'version': 'v1', 'timestamp': '2019-09-02T00:00:00.000Z', 'event': {'id_bpl_market': '10000032', 'num_rows': 5}}, {'version': 'v1', 'timestamp': '2019-09-02T00:00:00.000Z', 'event': {'id_bpl_market': '10003085', 'num_rows': 5}}]\n"
     ]
    }
   ],
   "source": [
    " \n",
    "query = PyDruid('http://bislave02.dev3.bloombergpolarlake.com:8082', 'druid/v2/')\n",
    " \n",
    " \n",
    "weekly_counts = query.groupby(\n",
    "    datasource = \"bpl_bdp_publication_dev3.bpl_market_master_druid\",\n",
    "    granularity = \"week\",\n",
    "    intervals = \"2018-05-01T00:00/2020-08-28T23:59\",\n",
    "    aggregations = {\"num_rows\": count(None)},\n",
    "    dimensions = [\"id_bpl_market\"],\n",
    "    having = Having(type=\"greaterThan\", aggregation=\"num_rows\", value=4),\n",
    "        limit_spec={\n",
    "        \"type\": \"default\",\n",
    "        \"limit\": 25,\n",
    "        \"columns\" : [\"num_rows\"]\n",
    "    }\n",
    ")\n",
    " \n",
    "print (weekly_counts)  # Do this if you want to see the raw JSON"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Any records updated more than once a day in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "query = PyDruid('http://bislave02.dev3.bloombergpolarlake.com:8082', 'druid/v2/')\n",
    " \n",
    " \n",
    "weekly_counts = query.groupby(\n",
    "    datasource = \"bpl_bdp_publication_dev3.bpl_market_master_druid\",\n",
    "    granularity = \"day\",\n",
    "    intervals = \"2018-05-01T00:00/2020-08-28T23:59\",\n",
    "    aggregations = {\"num_rows\": count(None)},\n",
    "    dimensions = [\"id_bpl_market\"],\n",
    "    having = Having(type=\"greaterThan\", aggregation=\"num_rows\", value=1),\n",
    "        limit_spec={\n",
    "        \"type\": \"default\",\n",
    "        \"limit\": 50,\n",
    "        \"columns\" : [\"num_rows\"]\n",
    "    }\n",
    ")\n",
    " \n",
    "print (weekly_counts)  # Do this if you want to see the raw JSON"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attempt to query unique file IDs per day\n",
    "Not Working currently !\n",
    "A topN query might be better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "Missing parentheses in call to 'print'. Did you mean print(weekly_counts  # Do this if you want to see the raw JSON)? (<ipython-input-11-35d5166c7fc1>, line 12)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-11-35d5166c7fc1>\"\u001b[1;36m, line \u001b[1;32m12\u001b[0m\n\u001b[1;33m    print weekly_counts  # Do this if you want to see the raw JSON\u001b[0m\n\u001b[1;37m                      ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m Missing parentheses in call to 'print'. Did you mean print(weekly_counts  # Do this if you want to see the raw JSON)?\n"
     ]
    }
   ],
   "source": [
    " \n",
    "query = PyDruid('http://biambari01.dev3.bloombergpolarlake.com:8082', 'druid/v2/')\n",
    " \n",
    " \n",
    "weekly_counts = query.timeseries(\n",
    "    datasource = \"ssb_druid.bloomberg_mtge_druid_cube\",\n",
    "    granularity = \"week\",\n",
    "    intervals = \"2018-01-01T00:00/2018-02-28T23:59\",\n",
    "    aggregations =  {\"unique_files\":countDistinct(\"system_id\")},\n",
    ")\n",
    " \n",
    "print weekly_counts  # Do this if you want to see the raw JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
