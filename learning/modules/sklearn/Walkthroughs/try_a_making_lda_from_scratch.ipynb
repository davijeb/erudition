{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_wine\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "np.set_printoptions(precision=4)\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Linear Discriminant Analysis From Scratch\n",
    "\n",
    "Use sklearn Wine dateset to run an LDA algo against"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "wine = load_wine()\n",
    "X = pd.DataFrame(wine.data, columns=wine.feature_names)\n",
    "y = pd.Categorical.from_codes(wine.target, wine.target_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dataframe with both features and classes\n",
    "\n",
    "df = X.join(pd.Series(y, name='wine'))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Discriminant Analysis Steps\n",
    "\n",
    "https://towardsdatascience.com/linear-discriminant-analysis-in-python-76b8b17817c2\n",
    "\n",
    "Linear Discriminant Analysis can be broken up into the following steps:\n",
    "- Compute the within class and between class scatter matrices\n",
    "- Compute the eigenvectors and corresponding eigenvalues for the scatter matrices\n",
    "- Sort the eigenvalues and select the top k\n",
    "- Create a new matrix containing eigenvectors that map to the k eigenvalues\n",
    "- Obtain the new features (i.e. LDA components) by taking the dot product of the data and the matrix from step 4\n",
    "\n",
    "Linear discriminant analysis of the form discussed above has its roots in an approach developed by the famous statistician R.A. Fisher, who arrived at linear discriminants from a different perspective. He was interested in finding a linear projection for data that maximizes the variance between classes relative to the variance for data from the same class. This approach is known as Fisherâ€™s linear discriminant analysis, and can be formulate for two classes or multiple classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Within Class Scatter Matrix\n",
    "We calculate the within class scatter matrix using the following formula.\n",
    "\n",
    "A scatter matrix is a statistic that is used to make estimates of the covariance matrix, for instance of the multivariate normal distribution.\n",
    "\n",
    "Lets define the scatter matrix (within) $S_W$ as\n",
    "\n",
    "$$S_W = \\sum_{i=1}^{c}{S_i}$$\n",
    "\n",
    "where c is the total number of distinct classes and\n",
    "\n",
    "$$S_i = \\sum_{x\\in D_i}^{c}{(x-m_i)(x-m_i)^T}$$\n",
    "\n",
    "$$m_i = \\frac{1}{n_i} \\sum_{x\\in D_i}^{n}{x_k}$$\n",
    "\n",
    "where x is a sample (i.e. row) and n is the total number of samples with a given class.\n",
    "\n",
    "For every class, we create a vector with the means of each feature.\n",
    "\n",
    "The full equation can be written as\n",
    "\n",
    "$$S_W = \\sum_{i=1}^{c}{\\sum_{x\\in D_i}^{c}{(x-m_i)(x-m_i)^T}}$$\n",
    "\n",
    "$$S_W = \\sum_{i=1}^{c}{\\sum_{x\\in D_i}^{c}{(x-\\frac{1}{n_i} \\sum_{x\\in D_i}^{n}{x_k})(x-\\frac{1}{n_i} \\sum_{x\\in D_i}^{n}{x_k})^T}}$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_feature_means = pd.DataFrame(columns=wine.target_names)\n",
    "class_feature_means"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STEP 1 - Create vector of feature means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c, rows in df.groupby('wine'):\n",
    "    class_feature_means[c] = (rows.mean())\n",
    "class_feature_means['class_0'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STEP 1 - Within class scatter matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "within_class_scatter_matrix = np.zeros((13,13))\n",
    "\n",
    "for c, rows in df.groupby('wine'):\n",
    "    s = np.zeros((13,13))\n",
    "    rows = rows.drop(columns=['wine'])\n",
    "    for i, x in rows.iterrows():\n",
    "        mc = class_feature_means[c]\n",
    "        r = ((x - mc).values.reshape(13,1))\n",
    "        res = r.dot(r.T)\n",
    "        s+=res\n",
    "    within_class_scatter_matrix+=s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STEP 1 - Between class scatter matrix\n",
    "\n",
    "We calculate the between class scatter matrix using the following formula:\n",
    "\n",
    "$$S_b = \\sum_{i=1}^{c}N_i{(m_i-m)(m_i-m)^T}$$\n",
    "\n",
    "where\n",
    "\n",
    "$$m_i = \\frac{1}{n_i} \\sum_{x\\in D_i}^{n}{x_k}$$\n",
    "\n",
    "and\n",
    "\n",
    "$$m = \\frac{1}{n} \\sum_{i}^{n}{x_k}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the feature means\n",
    "feature_means= df.mean()\n",
    "between_sclass_catter_matrix = np.zeros((13,13))\n",
    "\n",
    "for c in class_feature_means:\n",
    "    n = df[df['wine']==c].shape[0]\n",
    "    \n",
    "    mc  = class_feature_means[c].values.reshape(13,1)\n",
    "    m = feature_means.values.reshape(13,1)\n",
    "    \n",
    "    between_sclass_catter_matrix += n * (mc-m).dot((mc-m).T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2\n",
    "\n",
    "Compute the eigenvectors and corresponding eigenvalues for the scatter matrices\n",
    "We solve the generalised eigenvalue problem for\n",
    "\n",
    "$$S_W^{-1}S_B$$\n",
    "\n",
    "to obtain the linear discriminants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eig_val, eig_vec = np.linalg.eig(np.linalg.inv(within_class_scatter_matrix).dot(between_sclass_catter_matrix))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The eigenvectors with the highest eigenvalues carry the most information about the distribution of the data. Thus, we sort the eigenvalues from highest to lowest and select the first $k$ eigenvectors. In order to ensure that the eigenvalue maps to the same eigenvector after sorting, we place them in a temporary array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs = [(np.abs(eig_val[i]), eig_vec[i]) for i in range(len(eig_val))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(eig_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eig_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
