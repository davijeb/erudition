{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import os, sys, plotly.graph_objects as go\n",
    "import plotly.figure_factory as ff\n",
    "module_path = os.path.abspath(os.path.join('../../../../../..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path) \n",
    "\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn import datasets \n",
    "\n",
    "from erudition.learning.helpers.plots.plotly_render import render, scatter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y = datasets.make_regression(n_features=1, n_samples=100, noise=25)\n",
    "\n",
    "render(go.Figure(data=[scatter(x[:,0], y, 'Data', mode='markers')]), title='Random scatter')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hypothesis\n",
    "\n",
    "Let's define our linear hypothesis to fit the feature space:\n",
    "\n",
    "$$h_\\theta(x) = \\theta_0 + \\theta_1.x$$\n",
    "\n",
    "We then use RMSE to find the best possible fit and then define the cost function. The aim here is to minimize the cost function with regard to the theta paramaters\n",
    "\n",
    "$$J(\\theta) = \\frac {1}{2n} \\sum_{i=1}^n(h_\\theta(x_i) - y_i)^2$$\n",
    "\n",
    "As we vary the parameter values we will see an increase or decrease in the cost function. The goal here is to find the minimum of the cost function.\n",
    "\n",
    "By taking the partial derivative of the cost function with regard to each parameter value we have the slope of the cost function curve at that point. As the cost function is convex then by walking back down the slope by a certain amount we will move towards the local minima.\n",
    "\n",
    "$$\\frac{\\partial J(\\theta_0, \\theta_1)}{\\partial \\theta_0} = \\frac{\\partial}{\\partial \\theta_0}\\frac {1}{2n} \\sum_{i=1}^n(h_\\theta(x_i) - y_i)^2 $$\n",
    "\n",
    "Then, using the sum rule we can write:\n",
    "\n",
    "$$\\frac{\\partial J(\\theta_0, \\theta_1)}{\\partial \\theta_0} = \\frac {1}{2n} \\sum_{i=1}^n\\frac{\\partial}{\\partial \\theta_0}(h_\\theta(x_i) - y_i)^2 $$\n",
    "\n",
    "Next we use the power rule to get:\n",
    "\n",
    "$$\\frac{\\partial J(\\theta_0, \\theta_1)}{\\partial \\theta_0} = \\frac {1}{2n} \\sum_{i=1}^n2(h_\\theta(x_i) - y_i)\\frac{\\partial}{\\partial \\theta_0}(h_\\theta(x_i) - y_i)$$\n",
    "\n",
    "\n",
    "And finally, differentiating with regard to $\\theta_n$ gives:\n",
    "\n",
    "$$\\frac{\\partial J(\\theta_0, \\theta_1)}{\\partial \\theta_0} = \\frac {1}{n} \\sum_{i=1}^n(h_\\theta(x_i) - y_i)$$\n",
    "\n",
    "and\n",
    "\n",
    "$$\\frac{\\partial J(\\theta_0, \\theta_1)}{\\partial \\theta_1} = \\frac {1}{n} \\sum_{i=1}^n(h_\\theta(x_i) - y_i).x_i$$\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost_function(X, y, thetas):\n",
    "    dt = np.dot(X,thetas) - y\n",
    "    \n",
    "    return np.dot(dt.T,dt)/(2*len(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(X, y, thetas, alpha, iters=1000):\n",
    "    \n",
    "    for _ in range(iters):\n",
    "        thetas -= alpha * np.dot(X.T, (np.dot(X, thetas) - y)) / 100\n",
    "    return thetas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first create a 100 x 2 matrix with 1s in the first column\n",
    "X = np.ones((100,2))\n",
    "X[:,1] = x[:,0]\n",
    "\n",
    "# make y a 2d matrix\n",
    "yr = y.reshape(100,1)\n",
    "\n",
    "# set the initial values for the parameters\n",
    "thetas = np.zeros((2,1))\n",
    "\n",
    "# set the learning rate\n",
    "alpha = 0.2\n",
    "\n",
    "# set the number of iterations for the gradient descent algo\n",
    "iters = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta_grad = gradient_descent(X, yr, thetas, alpha, iters=500)\n",
    "theta_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_range = np.arange(-2,2, 0.01)\n",
    "\n",
    "render(\n",
    "    go.Figure(\n",
    "        data=[\n",
    "            scatter(x[:,0], y, 'Data', mode='markers', opacity=1),\n",
    "            scatter(x_range, 6.11 + 82.75*x_range, 'Data', mode='lines', color='pink', opacity=1)\n",
    "        ]), \n",
    "    title='Actually From Scratch'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
