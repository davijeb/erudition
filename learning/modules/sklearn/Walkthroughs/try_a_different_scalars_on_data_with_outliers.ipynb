{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature 0 (median income in a block) and feature 5 (number of households) of the California housing dataset have very different scales and contain some very large outliers. These two characteristics lead to difficulties to visualize the data and, more importantly, they can degrade the predictive performance of many machine learning algorithms. Unscaled data can also slow down or even prevent the convergence of many gradient-based estimators.\n",
    "\n",
    "Indeed many estimators are designed with the assumption that each feature takes values close to zero or more importantly that all features vary on comparable scales. In particular, metric-based and gradient-based estimators often assume approximately standardized data (centered features with unit variances). A notable exception are decision tree-based estimators that are robust to arbitrary scaling of the data.\n",
    "\n",
    "This example uses different scalers, transformers, and normalizers to bring the data within a pre-defined range.\n",
    "\n",
    "Scalers are linear (or more precisely affine) transformers and differ from each other in the way to estimate the parameters used to shift and scale each feature.\n",
    "\n",
    "QuantileTransformer provides non-linear transformations in which distances between marginal outliers and inliers are shrunk. PowerTransformer provides non-linear transformations in which data is mapped to a normal distribution to stabilize variance and minimize skewness.\n",
    "\n",
    "Unlike the previous transformations, normalization refers to a per sample transformation instead of a per feature transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import os, sys, plotly.graph_objects as go\n",
    "import plotly.figure_factory as ff\n",
    "module_path = os.path.abspath(os.path.join('../../../../..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path) \n",
    "\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn import datasets \n",
    "from erudition.learning.modules.sklearn.GeneralizedLinearModels.helper import helper\n",
    "from erudition.learning.helpers.plots.plotly_render import render, scatter\n",
    "\n",
    "from sklearn.datasets import fetch_california_housing, load_boston"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_boston()\n",
    "\n",
    "X_full, y_full = dataset.data, dataset.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take only 2 features to make visualization easier\n",
    "# Feature of 0 has a long tail distribution.\n",
    "# Feature 5 has a few but very large outliers.\n",
    "\n",
    "X = X_full[:, [0, 5]]\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(X): \n",
    "    colorscale = ['#AAAAAA', '#FFFFFF', '#FFFFFF', (1, 1, 0.2), (0.98,0.98,0.98)]\n",
    "\n",
    "    fig = ff.create_2d_density(\n",
    "        X[:,0], X[:,1],\n",
    "        point_size=3,\n",
    "        colorscale = 'Greys',\n",
    "        hist_color=colorscale\n",
    "    )\n",
    "\n",
    "    render(fig, title='Distribution', width=900)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Standard Scalar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scalar = StandardScaler()\n",
    "X_scaled = scalar.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(X_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quantile Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import QuantileTransformer\n",
    "\n",
    "trans = QuantileTransformer(output_distribution='normal')\n",
    "X_trans = trans.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(X_trans)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PowerTransformer\n",
    "\n",
    "Apply a power transform featurewise to make data more Gaussian-like.\n",
    "\n",
    "Power transforms are a family of parametric, monotonic transformations that are applied to make data more Gaussian-like. This is useful for modeling issues related to heteroscedasticity (non-constant variance), or other situations where normality is desired.\n",
    "\n",
    "Currently, PowerTransformer supports the Box-Cox transform and the Yeo-Johnson transform. The optimal parameter for stabilizing variance and minimizing skewness is estimated through maximum likelihood.\n",
    "\n",
    "Box-Cox requires input data to be strictly positive, while Yeo-Johnson supports both positive or negative data.\n",
    "\n",
    "By default, zero-mean, unit-variance normalization is applied to the transformed data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PowerTransformer\n",
    "\n",
    "trans = PowerTransformer(method='box-cox')\n",
    "X_trans = trans.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(X_trans)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
